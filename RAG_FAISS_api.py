

#  STEP 1: è¼‰å…¥ PDF ä¸¦åˆ†æ®µ

import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import re
def load_pdf(file_path):
    doc = fitz.open(file_path)
    paragraphs = []
    for page in doc:
        text = page.get_text()
        paragraphs.extend([p.strip() for p in re.split(r'(?<=[ã€‚ï¼ï¼Ÿ!?])\s+|\n+', text) if p.strip()])
    return paragraphs



def embedding_paragraph(model, paragraphs):   
    # STEP 2: å°‡æ®µè½è½‰æˆå‘é‡
    embeddings = model.encode(paragraphs)  #  å°æ¯æ®µåš embedding
    return embeddings

def create_faiss_index(embeddings):
    # STEP 3: å»ºç«‹ FAISS å‘é‡ç´¢å¼•ï¼ŒæŠŠ embeddings åŠ é€² FAISS å‘é‡è³‡æ–™åº«
    dimension = embeddings.shape[1]  #åœ¨FAISSå‰µé€ ä¸€å€‹embeddingsç¶­åº¦çš„å¤§å°
    index = faiss.IndexFlatIP(dimension)  #ä½¿ç”¨ L2è·é›¢(IndexHNSWFlatã€IndexPQ...)
    index.add(np.array(embeddings))
    return index

def search_similar(index,model,paragraphs,query, k=10):         # STEP 4: æŸ¥è©¢ç›¸é—œæ®µè½ï¼Œè¿”å›žå‰ä¸‰ç›¸ä¼¼çš„ç‰‡æ®µ
    query_embedding = model.encode([query])
    D, I = index.search(np.array(query_embedding), k)
    
    print("D,I=",D,I)
    for i, idx in enumerate(I[0]):
        print(f"ç›¸ä¼¼åº¦: {D[0][i]:.4f} å…§å®¹: {paragraphs[idx]}")
    return [paragraphs[i] for i in I[0]]

# STEP 5: ä½¿ç”¨ OpenAI å›žç­”å•é¡Œ
 
# import openai

# openai.api_key = "your-api-key"  # <-- è«‹å¡«å…¥ä½ çš„ API é‡‘é‘°

# def ask_gpt(query, context):
#     prompt = f""" You need to answer the question in the sentence as same as in the  pdf content. . 
        # Given below is the context and question of the user.
        # context = {context}
        # question = {query}
        # if the answer is not in the pdf , answer "i donot know what the hell you are asking about"
        #  """

#     response = openai.ChatCompletion.create(
#         model="gpt-3.5-turbo",
#         messages=[{"role": "user", "content": prompt}]
#     )
#     return response["choices"][0]["message"]["content"]

def greet(query,pdf_path="oreilly-technical-guide-understanding-etl.pdf" ):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    paragraphs = load_pdf(pdf_path)
    embeddings = embedding_paragraph(model, paragraphs)
    index = create_faiss_index(embeddings)
    relevant = search_similar(index, model, paragraphs,query, k=10)
    context = "\n\n---\n\n".join(relevant)  # æ¯æ®µåŠ åˆ†éš”ç·š
    print("=== æŸ¥è©¢çµæžœ ===\n")
    print(context.strip())
    return(context.strip())

    

def build_API():
    import gradio as gr
    # Use the gradio library to display a user interface for your user to interact with.
    pdf_path = "oreilly-technical-guide-understanding-etl.pdf" 
    demo = gr.Interface(
        fn=greet,
        inputs=gr.Textbox(lines=2, placeholder="è¼¸å…¥ä½ çš„å•é¡Œ..."),
        outputs="text",
        title="ðŸ“˜ PDF å•ç­”æœå°‹",
        description="è¼¸å…¥å•é¡Œï¼Œæˆ‘æœƒå¹«ä½ å¾ž PDF ä¸­æ‰¾å‡ºæœ€ç›¸é—œçš„æ®µè½"
    )

    # Launch the user demo - This can be done directly in your colab notebook. On your local notebook, you can also give a personalized localhost:port address.
    demo.launch(share=True,debug=False)



def main():
     
    # query = "ETL or ELT, which is better?"
    build_API()
    
    # answer = ask_gpt(query, context)
    # print("\nAnswer:\n", answer)



if __name__ == "__main__":
    main()



